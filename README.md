### Descri√ß√£o do Reposit√≥rio: Classificador MNIST com Rede Neural de 2 Camadas e 128 Neur√¥nios
üö® O c√≥digo de MINST_regularized.ipynb usa cupy, acelerado via GPU. Para fazer o c√≥digo funcionar voc√™ ter√° de trocar para numpy. \
Este reposit√≥rio cont√©m uma implementa√ß√£o completa de uma rede neural de duas camadas, projetada para classificar d√≠gitos manuscritos do conjunto de dados MNIST. A rede √© composta por uma camada oculta com 128 neur√¥nios e utiliza as fun√ß√µes de ativa√ß√£o ReLU e Softmax. O modelo √© treinado usando a t√©cnica de backpropagation com a fun√ß√£o de perda de entropia cruzada.

#### Fun√ß√µes Implementadas:

1. **`relu(x)`**:
   - **Descri√ß√£o**: Calcula a fun√ß√£o de ativa√ß√£o ReLU, que introduz n√£o-linearidade ao modelo. ReLU √© definida como max(0, x).
   - **Par√¢metros**:
     - `x`: Array NumPy com os valores de entrada.

2. **`relu_derivative(x)`**:
   - **Descri√ß√£o**: Calcula a derivada da fun√ß√£o ReLU, utilizada no processo de backpropagation.
   - **Par√¢metros**:
     - `x`: Array NumPy com os valores de entrada.

3. **`softmax(x)`**:
   - **Descri√ß√£o**: Aplica a fun√ß√£o de ativa√ß√£o Softmax, que √© usada na √∫ltima camada de redes neurais para classifica√ß√£o multiclasse. Essa fun√ß√£o normaliza as sa√≠das em uma distribui√ß√£o de probabilidade.
   - **Par√¢metros**:
     - `x`: Array NumPy com os valores de entrada.

4. **`cross_entropy_loss(predictions, targets)`**:
   - **Descri√ß√£o**: Calcula a perda de entropia cruzada, uma m√©trica de perda comum para problemas de classifica√ß√£o.
   - **Par√¢metros**:
     - `predictions`: Previs√µes do modelo.
     - `targets`: R√≥tulos verdadeiros.

5. **`predict(x, W1, b1, W2, b2)`**:
   - **Descri√ß√£o**: Realiza a predi√ß√£o usando os par√¢metros da rede. Passa os dados de entrada pela rede para obter as previs√µes.
   - **Par√¢metros**:
     - `x`: Dados de entrada.
     - `W1, b1, W2, b2`: Pesos e bias da rede.

6. **`initialize_parameters(n_input=784, n_hidden=128, n_output=10)`**:
   - **Descri√ß√£o**: Inicializa os pesos e bias da rede neural com valores aleat√≥rios pequenos.
   - **Par√¢metros**:
     - `n_input`: N√∫mero de neur√¥nios na camada de entrada (784 para imagens MNIST).
     - `n_hidden`: N√∫mero de neur√¥nios na camada oculta.
     - `n_output`: N√∫mero de neur√¥nios na camada de sa√≠da (10 para d√≠gitos de 0 a 9).

7. **`forward_propagation(x, W1, b1, W2, b2)`**:
   - **Descri√ß√£o**: Executa uma passagem para frente, calculando as sa√≠das da rede a partir das entradas.
   - **Par√¢metros**:
     - `x`: Dados de entrada.
     - `W1, b1, W2, b2`: Pesos e bias da rede.

8. **`backward_propagation(x, y, W1, b1, W2, b2, a1, y_hat)`**:
   - **Descri√ß√£o**: Executa o backpropagation para calcular os gradientes dos pesos e bias, baseando-se no erro entre as previs√µes e os r√≥tulos verdadeiros.
   - **Par√¢metros**:
     - `x`: Dados de entrada.
     - `y`: R√≥tulos verdadeiros.
     - `W1, b1, W2, b2, a1, y_hat`: Pesos, bias e ativa√ß√µes da rede.

9. **`train(x_train, y_train, W1, b1, W2, b2, eta=0.01, epochs=1000)`**:
   - **Descri√ß√£o**: Treina a rede usando os dados de treinamento, ajustando os pesos e bias para minimizar a perda.
   - **Par√¢metros**:
     - `x_train`: Dados de treinamento.
     - `y_train`: R√≥tulos de treinamento.
     - `W1, b1, W2, b2`: Pesos e bias da rede.
     - `eta`: Taxa de aprendizagem.
     - `epochs`: N√∫mero de √©pocas de treinamento.

10. **`load_mnist()`**:
    - **Descri√ß√£o**: Carrega os dados do conjunto de dados MNIST, normalizando os valores de pixel e convertendo os r√≥tulos em one-hot encoding.
    - **Sa√≠da**: Retorna os conjuntos de dados de treino e teste.

#### Diagrama da Rede Neural:

A seguir est√° o diagrama da rede neural:

```
[ 784 ] -- [ 128 ] -- ReLU -- [ 10 ] -- Softmax
```

- **[784]**: Camada de entrada, uma para cada pixel em imagens MNIST de 28x28.
- **[128]**: Camada oculta com 128 neur√¥nios.
- **ReLU**: Fun√ß√£o de ativa√ß√£o ReLU usada ap√≥s a primeira camada oculta.
- **[10]**: Camada de sa√≠da com 10 neur√¥nios, um para cada classe de d√≠gito (0-9).
- **Softmax**: Fun√ß√£o de ativa√ß√£o Softmax usada para normalizar as sa√≠das em uma distribui√ß√£o de probabilidade.

Este reposit√≥rio oferece uma base s√≥lida para entender e explorar redes neurais aplicadas ao problema cl√°ssico de reconhecimento de d√≠gitos escritos √† m√£o.
